---
title: "Post 1"
author: "Devon Brice"
date: "December 11, 2018"
output: html_document
---



**In my first Post, I will explore a data set from the UCI Machine Learning Repository about wine. This data is the result of a chemical analysis of 178 wines grown in the same region in Italy. In this Post, I will look at clustering techniques to find the best clusters for these 178 wines. Clustering into groups based on the wine's chemistry would be beneficila for chemical testing and flavor development.**

```{r wine}

#read in data
wine <- read.delim("winedata.txt", sep=",", header = FALSE)
wine[,1]<-NULL
colnames(wine) <- c("Alcohol", "Malic_acid", "Ash", "Alcalinity_ash", "Magnesium", "Total_phenols", "Flavanoids", "Nonflavanoid_phenols", "Proanthocyanins", "Color_intensity", "Hue", "OD280/OD315_diluted_wines", "Proline")

#add ID for each wine
wine$ID <- seq(from=1, to=178, by=1)

```


#Hierarchal Clustering technique
```{r}
#scale the data
data.scaled <- scale(wine[,1:13])


#Create distance matirx with Euclidean distance
d <- dist(data.scaled, method = "euclidean") 

#Fit using Ward method
fit <- hclust(d, method="ward.D2")

#Plot Dendrogram
plot(fit, labels=FALSE)
rect.hclust(fit, k=3, border="red") 


```

**According to the dendrogram, the best fit for the number of clusters is 3. This is shown by the distance in the height before three clusters merges into two clusters. I can further confirm this number by using K-Means clustering and cross-validating k. **



#K-means Clustering method
```{r}

#kmeans
#crossvalidating k with 
SSEs <- rep(NA,10) # a vector to store SSEs for different k's 

for(k in 1:13){    
  fit <- kmeans(wine[,1:13],k)    
  SSEs[k] <- fit$tot.withinss } 
par(mar=c(4,4,1,1))
plot(1:13,SSEs,type="b",xlab="Number of Clusters")


```


**In the code above, I analyze the Sum of Squared Errors within the clusters. I would chose a k value of 3 since it is approximately the "elbow" of the curve. The SSE is decreasing still but continuing to increase k will eventually give diminishing returns on the improvement of the clusters. At around k=3 and above, it is decreasing at a slower and slower rate.**


```{r}
#Fitting K-means with 3 clusters
fit2 <- kmeans(wine[,1:13],3)

#Centers of 3 clusters
fit2$centers

```

##Defining each Cluster

**Cluster 1** These wines have the highest alcohol content and the lowest Malic_acid. These are also the wines see the lowest levels in Alcalinity Ash and highest levels in Magnesium and Proline.  Additionally, these wines have the most potent color intensity and Hue.

**Cluster 2** This cluster represents the wines that are "average" in these chemicals among the 178 wines in the analysis. These wines do, thouhg, have the lowest levels of Flavanoids and OD280/OD315_diluted_wines as well as teh lowest levels of Hue.

**Cluster 3** These wines have the lowest alcohol content and the lowest levels of Magnesium. These are also the wines see the lowest levels in Proanthocyanins and Proline.  ON the other hand, these wines have the highest levels in Alcalinity_ash.




#Visualizations
```{r}
library(cluster)

plot(wine[c("Alcohol", "Proline")], col = fit2$cluster); points(fit2$centers[,c("Alcohol", "Proline")], col = 1:3, pch = 8, cex = 2)

```


** In the graph above, I have plotted the Alcohol levels of the wines against the Proline levels. The clusters are separated by color with the center of each cluster signified by a star. With these two variables, you can clearly see the clusters and how they contain different chemical levels for these wines.**


```{r}

clusplot(wine,fit2$cluster, color=TRUE, shade=TRUE, 
         labels=2, lines=0)
```



**The clusplot uses PCA to draw the data, which uses the first two principal components to explain the data. Principal components are the axes that along them the data has the most variability. In this plot, these two components explain about 57% of the variablity within the points of these wines.  **

